[package]
name = "ghost"
version = "0.11.2"
description = "Private Local AI Superpowers for Your OS"
authors = ["Ghost Contributors <https://github.com/ghostapp-ai>"]
license = "MIT"
repository = "https://github.com/ghostapp-ai/ghost"
homepage = "https://github.com/ghostapp-ai/ghost"
edition = "2021"
rust-version = "1.80"

[lib]
name = "ghost_lib"
crate-type = ["staticlib", "cdylib", "rlib"]

[features]
default = []

# GPU features for llama.cpp chat inference (runtime auto-detection)
vulkan = ["llama-cpp-2/vulkan"]
metal = ["llama-cpp-2/metal", "candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]
cuda = ["llama-cpp-2/cuda", "candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
# Candle-only features (embeddings)
accelerate = ["candle-core/accelerate", "candle-nn/accelerate", "candle-transformers/accelerate"]

[build-dependencies]
tauri-build = { version = "2", features = [] }

[dependencies]
# Tauri (tray-icon only on desktop via target-specific dep)
tauri = { version = "2", features = ["tray-icon", "image-png"] }
tauri-plugin-opener = "2"

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Database
rusqlite = { version = "0.32", features = ["bundled", "vtab"] }

# Async
tokio = { version = "1", features = ["full"] }
futures = "0.3"

# File watching (desktop only — see target-specific deps below)
# notify and notify-debouncer-mini moved to [target.'cfg(desktop)'.dependencies]

# Text extraction
lopdf = "0.39"
calamine = "0.33"

# HTTP client (for Ollama — rustls avoids OpenSSL cross-compilation issues on Android/iOS)
# NOTE: Use rustls-no-provider to avoid aws-lc-sys (fails on MSVC with __builtin_bswap*).
# Ring is installed as the default CryptoProvider at startup in lib.rs.
reqwest = { version = "0.13", default-features = false, features = ["json", "rustls-no-provider"] }

# TLS crypto provider — ring compiles cleanly on all platforms (MSVC, GCC, Clang)
rustls = { version = "0.23", default-features = false, features = ["ring", "std", "tls12", "logging"] }

# Vector search
sqlite-vec = "0.1.7-alpha.10"

# DOCX extraction
zip = { version = "7", default-features = false, features = ["deflate"] }

# Error handling
thiserror = "2"
anyhow = "1"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# Native AI — Embeddings (Candle BERT, kept for all-MiniLM-L6-v2)
candle-core = "0.9"
candle-nn = "0.9"
candle-transformers = "0.9"
hf-hub = { version = "0.4", default-features = false, features = ["tokio", "ureq", "rustls-tls"] }
tokenizers = "0.22"
encoding_rs = "0.8"

# MCP Protocol (Phase 1.5 — Protocol Bridge)
rmcp = { version = "0.16", features = ["server", "client", "transport-io", "transport-child-process", "transport-streamable-http-server", "transport-streamable-http-client-reqwest"] }
schemars = "1"
axum = "0.8"
tower = "0.5"

# AG-UI streaming support
async-stream = "0.3"

# YAML parsing (SKILL.md frontmatter)
serde_yaml = "0.9"

# Utilities
sha2 = "0.10"
hex = "0.4"
dirs = "6"
chrono = "0.4"
# Global shortcut plugin (desktop only — see target-specific deps below)
# tauri-plugin-global-shortcut moved to [target.'cfg(desktop)'.dependencies]

# --- Desktop-only dependencies (not compiled for iOS/Android) ---
# Tauri sets cfg(desktop) for Windows, macOS, and Linux targets.
# These crates don't compile or aren't useful on mobile.
[target.'cfg(not(any(target_os = "android", target_os = "ios")))'.dependencies]
tauri = { version = "2", features = ["tray-icon", "image-png"] }
notify = "7"
notify-debouncer-mini = "0.5"
tauri-plugin-global-shortcut = "2.3.1"
# Chat inference via llama.cpp (requires C++ toolchain, desktop only)
llama-cpp-2 = "0.1"
# Auto-updater + process control (desktop only — not needed on mobile)
tauri-plugin-updater = "2"
tauri-plugin-process = "2"

# ── Release profile: optimized for CI build speed ──────────────────
# strip = true removes debug symbols → faster linking + smaller binary
# lto = "thin" gives ~10% runtime boost without fat-LTO compile cost
# codegen-units = 16 (default) keeps compile parallelism high
# opt-level = 2 is ~95% of opt-level 3 perf with faster compile
[profile.release]
strip = true
lto = "thin"
opt-level = 2
codegen-units = 16
